{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0b43ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error occurred for https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "HTTP error occurred for https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "HTTP error occurred for https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n",
      "Text analysis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import cmudict, stopwords\n",
    "import re\n",
    "\n",
    "# Function to count syllables in a word using the CMU Pronouncing Dictionary\n",
    "def syllable_count(word):\n",
    "    if word.lower() in cmudictionary:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in cmudictionary[word.lower()]])\n",
    "    else:\n",
    "        vowels = \"aeiouAEIOU\"\n",
    "        return sum(1 for char in word if char in vowels)\n",
    "\n",
    "# Load CMU Pronouncing Dictionary for syllable count\n",
    "cmudictionary = cmudict.dict()\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = \"Output_Data_Structure.xlsx\"\n",
    "\n",
    "# Load stop words from the provided folders\n",
    "stop_words_folder = \"StopWords\"\n",
    "stop_words = set()\n",
    "\n",
    "for filename in os.listdir(stop_words_folder):\n",
    "    with open(os.path.join(stop_words_folder, filename), 'r', encoding='latin-1') as file:\n",
    "        stop_words.update(file.read().splitlines())\n",
    "\n",
    "# Directories\n",
    "text_dir = \"ArticleTexts\"\n",
    "stopwords_dir = \"StopWords\"\n",
    "sentiment_dir = \"MasterDictionary\"\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "for directory in [text_dir, stopwords_dir, sentiment_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Load positive and negative words from the sentiment directory\n",
    "pos = set()\n",
    "neg = set()\n",
    "\n",
    "for files in os.listdir(sentiment_dir):\n",
    "    if files == 'positive-words.txt':\n",
    "        with open(os.path.join(sentiment_dir, files), 'r', encoding='ISO-8859-1') as f:\n",
    "            pos.update(f.read().splitlines())\n",
    "    else:\n",
    "        with open(os.path.join(sentiment_dir, files), 'r', encoding='ISO-8859-1') as f:\n",
    "            neg.update(f.read().splitlines())\n",
    "\n",
    "# Define personal pronouns\n",
    "personal_pronouns = [\"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\"]\n",
    "\n",
    "# Create an empty DataFrame to store the analysis results\n",
    "output_data = pd.DataFrame(columns=['URL_ID', 'URL', 'Positive Score', 'Negative Score', 'Polarity Score',\n",
    "                                     'Subjectivity Score', 'Average Sentence Length', 'Percentage of Complex Words',\n",
    "                                     'Fog Index', 'Average Number of Words Per Sentence', 'Complex Word Count',\n",
    "                                     'Word Count', 'Syllable Per Word', 'Personal Pronouns', 'Average Word Length'])\n",
    "\n",
    "# Define the path to your input data file\n",
    "input_file_path = \"Input.xlsx\"\n",
    "\n",
    "# Load input data for analysis\n",
    "input_data_analysis = pd.read_excel(input_file_path)\n",
    "\n",
    "# Loop through each row in the input data for analysis\n",
    "for index, row in input_data_analysis.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers, allow_redirects=True)\n",
    "        response.raise_for_status()  # Raise an exception for bad responses\n",
    "\n",
    "        # Use BeautifulSoup to parse HTML and extract only article title and text\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract article title\n",
    "        title = soup.find('h1').get_text() if soup.find('h1') else \"\"\n",
    "\n",
    "        # Extract article text \n",
    "        article_text = \"\"\n",
    "        try:\n",
    "            for p in soup.find_all('p'):\n",
    "                article_text += p.get_text()\n",
    "        except:\n",
    "            print(f\"Error extracting text from URL {url_id}\")\n",
    "\n",
    "        if article_text:\n",
    "            # Save the extracted article text to a text file with URL_ID as its file name\n",
    "            text_file_path = os.path.join(text_dir, f\"{url_id}.txt\")\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                text_file.write(title + '\\n' + article_text)\n",
    "\n",
    "            # Perform sentiment analysis and other tasks using the extracted text\n",
    "        \n",
    "\n",
    "            # Perform sentiment analysis using TextBlob\n",
    "            blob = TextBlob(article_text.lower())\n",
    "\n",
    "            # Filter out stop words and clean the text\n",
    "            words = [re.sub(r'[^\\w\\s]', '', word) for word in blob.words if word.lower() not in stop_words]\n",
    "\n",
    "            # Positive and Negative Score Calculation\n",
    "            positive_score = sum(1 for word in words if word in pos)\n",
    "            negative_score = sum(1 for word in words if word in neg)\n",
    "\n",
    "            # Polarity Score Calculation\n",
    "            rounded_polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "            rounded_polarity_score = max(-1, min(1, rounded_polarity_score))\n",
    "\n",
    "            # Subjectivity Score Calculation\n",
    "            subjectivity_score = (positive_score + abs(negative_score)) / (len(words) + 0.000001)\n",
    "\n",
    "            # Analysis of Readability\n",
    "            sentences = sent_tokenize(article_text)\n",
    "            avg_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
    "\n",
    "            # Percentage of Complex Words Calculation\n",
    "            complex_words = [word for word in words if syllable_count(word) > 2]\n",
    "            percentage_of_complex_words = len(complex_words) / len(words)\n",
    "\n",
    "            # Fog Index Calculation\n",
    "            fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
    "\n",
    "            # Average Number of Words Per Sentence Calculation\n",
    "            avg_words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "            # Complex Word Count Calculation\n",
    "            complex_word_count = len(complex_words)\n",
    "\n",
    "            # Word Count Calculation\n",
    "            word_count = len(words)\n",
    "\n",
    "            # Syllable Count Per Word Calculation\n",
    "            syllables_per_word = sum(syllable_count(word) for word in words) / len(words)\n",
    "\n",
    "            # Tokenize words using NLTK for personal pronouns count\n",
    "            tokenized_words = word_tokenize(article_text)\n",
    "\n",
    "            personal_pronouns_count = sum(1 for word in tokenized_words if word.lower() in personal_pronouns)\n",
    "\n",
    "            # Average Word Length Calculation\n",
    "            avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "\n",
    "            # Populate the analysis results dictionary\n",
    "            analysis_results = {\n",
    "                'URL_ID': url_id,\n",
    "                'URL': url,\n",
    "                'Positive Score': positive_score,\n",
    "                'Negative Score': negative_score,\n",
    "                'Polarity Score': rounded_polarity_score,\n",
    "                'Subjectivity Score': subjectivity_score,\n",
    "                'Average Sentence Length': avg_sentence_length,\n",
    "                'Percentage of Complex Words': percentage_of_complex_words,\n",
    "                'Fog Index': fog_index,\n",
    "                'Average Number of Words Per Sentence': avg_words_per_sentence,\n",
    "                'Complex Word Count': complex_word_count,\n",
    "                'Word Count': word_count,\n",
    "                'Syllable Per Word': syllables_per_word,\n",
    "                'Personal Pronouns': personal_pronouns_count,\n",
    "                'Average Word Length': avg_word_length\n",
    "            }\n",
    "\n",
    "            # Append the analysis results to the output_data DataFrame\n",
    "            output_data = output_data.append(analysis_results, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            print(f\"No article content found for URL {url}\")\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred for {url}: {http_err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from URL {url}: {e}\")\n",
    "\n",
    "# Save the analysis results to a new Excel file\n",
    "output_data.to_excel(output_file_path, index=False)\n",
    "\n",
    "# Print a message indicating the completion of text analysis\n",
    "print(\"Text analysis completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9b15a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
